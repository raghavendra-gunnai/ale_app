{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txf7grzMwC2R"
      },
      "source": [
        "# Alethea MLE Take Home Assignment\n",
        "\n",
        "Congratulations on moving to this step of the interview process! This assignment is meant to be a practical assessment of coding that you may expect to do at Alethea. We understand that take home assignments require work beyond your other current responsibilities, and we appreciate you taking the time and effort to answer the questions here.\n",
        "\n",
        "We ask that you use Python for the assignment, as that is one of the primary programming language we use. You may use any packages that you would like to answer the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohegiJQbwgKw"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data that you will be working with is a sample of posts about COVID-19 from December 2023. There are two datasets, `posts` and `accounts`, provided as TSVs and containing the following fields.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgYH3kcswgO4"
      },
      "source": [
        "### Posts\n",
        "|    Field   |   Type   |                               Description                              |\n",
        "|------------|----------|------------------------------------------------------------------------|\n",
        "| id         | str      | The unique identifier of the post                                      |\n",
        "| created_at | datetime | The time at which the post was made                                    |\n",
        "| author_id  | str      | The ID of the account that made the post                               |\n",
        "| is_repost  | bool     | Whether the post is a repost                                           |\n",
        "| text       | str      | The text of the post                                                   |\n",
        "| hashtags   | str      | The unique hashtags present in the post, each separated by a pipe `\\|` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lb5QAFLwgSn"
      },
      "source": [
        "### Accounts\n",
        "\n",
        "| Field       | Type     | Description                                                          |\n",
        "|-------------|----------|----------------------------------------------------------------------|\n",
        "| id          | str      | The unique identifier of the account                                 |\n",
        "| created_at  | datetime | The time at which the account was made                               |\n",
        "| screen_name | str      | The screen name of the account, also known as its handle or username |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmYosKQWwNYC"
      },
      "source": [
        "## Loading Data\n",
        "\n",
        "Please run the following `wget` commands to download the data into your environment. Then, run the Python cells to structure the data in the format that\n",
        "we will use for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "786C7Q53O047"
      },
      "outputs": [],
      "source": [
        "!wget https://alethea-take-home.s3.amazonaws.com/posts.tsv\n",
        "!wget https://alethea-take-home.s3.amazonaws.com/accounts.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdFdxtgVl1jE"
      },
      "outputs": [],
      "source": [
        "#!pip install polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH7a2a--wEte"
      },
      "outputs": [],
      "source": [
        "import polars as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq5KXFD-blof"
      },
      "outputs": [],
      "source": [
        "df_posts = (pl\n",
        "    .read_csv(\n",
        "        'posts.tsv',\n",
        "        separator='\\t',\n",
        "        try_parse_dates=True\n",
        "    ).with_columns(\n",
        "        hashtags=pl.col('hashtags').str.split('|')\n",
        "    )\n",
        ")\n",
        "\n",
        "df_accts = pl.read_csv(\n",
        "    'accounts.tsv',\n",
        "    separator='\\t'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RU6fLJ5Ep7T"
      },
      "outputs": [],
      "source": [
        "posts = df_posts.to_dicts()\n",
        "accts = df_accts.to_dicts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWxF8d_6wbNJ"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7UYsXKwSbm"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Given a set of posts, write a function that returns all of the accounts that authored posts using a certain hashtag, _not_ including reposts. The function should follow the provided model signature below.\n",
        "\n",
        "Use the function to determine the number of accounts that posted the hashtag #diedsuddenly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhpu0jgVwZ-4"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def get_hashtag_accounts(\n",
        "        posts: List[Dict],\n",
        "        hashtag: str\n",
        "    ) -> List[str]:\n",
        "    \"\"\"\n",
        "    From a set of posts, returns the IDs of all accounts that posted a\n",
        "    particular hashtag (excluding reposts)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    posts:\n",
        "        A list of dictionary records, where each record corresponds to one post.\n",
        "        Keys are metadata fields for a post, and values are the metadata itself\n",
        "    hashtag:\n",
        "        The hashtag by which to filter\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    accounts:\n",
        "        The IDs of all accounts that posted the `hashtag` (excluding reposts)\n",
        "    \"\"\"\n",
        "    account_ids = set()\n",
        "    for post in posts:\n",
        "        if not post['is_repost'] and post['hashtags'] and hashtag in post['hashtags']:\n",
        "            account_ids.add(post['author_id'])\n",
        "    return list(account_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xp4MRJAEe3C",
        "outputId": "aeb6a940-fb85-4ab0-e673-312bdacbeee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33 accounts posted the hashtag #diedsuddenly\n"
          ]
        }
      ],
      "source": [
        "filt_hashtag = '#diedsuddenly'\n",
        "filt_accts = get_hashtag_accounts(posts=posts, hashtag=filt_hashtag)\n",
        "\n",
        "print(f\"{len(filt_accts):,} accounts posted the hashtag {filt_hashtag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLM29VCkw5Ny"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Accounts that are all controlled by a single individual or organization using automation or semi-automation are often referred to as _sock puppets_. An indicator that a group of accounts may be sock puppets is that they all have similar screen names. For example, a disinformation campaign could pose as fake New York City news outlets with the screen names \"breaking_news_nyc\", \"breaking_news_ny\", and \"breaking_news_nyc2\".\n",
        "\n",
        "One way to measure the similarity of screen names is by using the [Damerau-Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance), an edit distance between two strings. It can be normalized by the maximum possible distance to produce the normalized Damerau-Levenshtein similarity, which is 1 if two strings are exactly the same and 0 if they are maximally different. A [performant implementation](https://rapidfuzz.github.io/RapidFuzz/Usage/distance/DamerauLevenshtein.html#normalized-similarity) of the similarity measure is available in the package RapidFuzz.\n",
        "\n",
        "Given a set of accounts, write a function that identifies all _unique pairs_ of accounts that have similar screen names. The function should follow the provided model signature below.\n",
        "\n",
        "Use the function to determine the number of _unique pairs_ of accounts that posted the hashtag #diedsuddenly (not including reposts), and that have screen names with a similarity greater than 0.8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDD2LOzbw5XX"
      },
      "outputs": [],
      "source": [
        "!pip install rapidfuzz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW3iA3KqxfKc"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from rapidfuzz.distance.DamerauLevenshtein import normalized_similarity\n",
        "\n",
        "def get_similar_screen_names(\n",
        "      accounts: List[Dict],\n",
        "      min_similarity: float\n",
        "    ) -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"\n",
        "    From a set of accounts, returns pairs of accounts that have similar screen\n",
        "    names according to the normalized Damerau-Levenshtein similarity\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    accounts:\n",
        "        A list of account records, where each record corresponds to one account.\n",
        "        Keys are metadata fields for an account, and values are the metadata\n",
        "        itself\n",
        "    min_similarity:\n",
        "        A value between 0 and 1, indicating the minimum similarity needed to\n",
        "        determine two accounts have similar screen names\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    similar_account_pairs:\n",
        "        A list of tuples of the format (account_id1, account_id2) indicating\n",
        "        which accounts have similar screen names\n",
        "    \"\"\"\n",
        "    similar_account_pairs = []\n",
        "    for account_1, account_2 in combinations(accounts, 2):\n",
        "        screen_name1 = account_1['screen_name']\n",
        "        screen_name2 = account_2['screen_name']\n",
        "        similarity = normalized_similarity(screen_name1, screen_name2)\n",
        "        if similarity > min_similarity:\n",
        "            similar_account_pairs.append((account_1['id'], account_2['id'], similarity))\n",
        "    return similar_account_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqG8EW9iHyqw",
        "outputId": "78cda842-adef-486a-a3fc-5136a2c72eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37 pairs of accounts that posted the hashtag #diedsuddenly have similar screen names\n"
          ]
        }
      ],
      "source": [
        "hashtag_accts = [acct for acct in accts if acct['id'] in filt_accts]\n",
        "sim_accts = get_similar_screen_names(accounts=hashtag_accts, min_similarity=0.8)\n",
        "\n",
        "print(f\"{len(sim_accts):,} pairs of accounts that posted the hashtag {filt_hashtag} have similar screen names\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSMNL_aXB5hM"
      },
      "source": [
        "# Question 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKCTWPS7w6FP"
      },
      "source": [
        "We have a simple \"model\" to identify all pairs of accounts that posted a particular hashtag and have similar screen names. Suppose that we want to be able to access it via an API. Specifically, we want to create an API endpoint to which we can make GET requests, where it accepts the following request parameters and yields the following respose:\n",
        "\n",
        "**Input**\n",
        "- `posts`: A list of post records, where each record corresponds to one post\n",
        "- `accounts`: A list of account records, where each record corresponds to one account\n",
        "- `hashtag`: The hashtag by which to filter\n",
        "- `min_similarity`: The minimum similarity to determine two screen names are similar\n",
        "\n",
        "**Output**\n",
        "- `similar_account_pairs`: A list of tuples of the format (`account_id1`, `account_id2`) indicating which accounts have similar screen names\n",
        "\n",
        "Please create an endpoint according to the specified parameters using Flask or FastAPI. All input and output should be formatted using JSON.\n",
        "\n",
        "Feel free to do this in a different environment, and submit a link to a Github repo with deployment instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g88Iwo6Tdgdv"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from itertools import combinations\n",
        "from rapidfuzz.distance.DamerauLevenshtein import normalized_similarity\n",
        "import ast\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def get_similar_account_pairs(posts, accounts, hashtag, min_similarity):\n",
        "    similar_account_pairs = []\n",
        "\n",
        "    # Filter posts by hashtag and exclude reposts\n",
        "    account_ids = set()\n",
        "    for post in posts:\n",
        "        print(post)\n",
        "        if not post['is_repost'] and post['hashtags'] and hashtag in post['hashtags']:\n",
        "            account_ids.add(post['author_id'])\n",
        "\n",
        "    # Extract screen names from accounts\n",
        "    hashtag_accts = [acct for acct in accounts if acct['id'] in list(account_ids)]\n",
        "\n",
        "    # Iterate through combinations of relevant accounts and calculate similarity\n",
        "    similar_account_pairs = []\n",
        "    for account_1, account_2 in combinations(hashtag_accts, 2):\n",
        "        screen_name1 = account_1['screen_name']\n",
        "        screen_name2 = account_2['screen_name']\n",
        "        similarity = normalized_similarity(screen_name1, screen_name2)\n",
        "        if similarity > min_similarity:\n",
        "            similar_account_pairs.append((account_1['id'], account_2['id']))\n",
        "\n",
        "    return similar_account_pairs\n",
        "\n",
        "# New route for health check\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({'status': 'ok'})\n",
        "\n",
        "@app.route('/get_similar_account_pairs', methods=['GET'])\n",
        "def api_get_similar_account_pairs():\n",
        "    # Get parameters from request\n",
        "    posts = ast.literal_eval(request.args.get('posts'))\n",
        "    accounts = ast.literal_eval(request.args.get('accounts'))\n",
        "    hashtag = request.args.get('hashtag')\n",
        "    min_similarity_str = request.args.get('min_similarity')\n",
        "    if min_similarity_str is None:\n",
        "        min_similarity = 0.8  # Default value\n",
        "    else:\n",
        "        min_similarity = float(ast.literal_eval(min_similarity_str))\n",
        "\n",
        "    # Call the function to get similar account pairs\n",
        "    similar_account_pairs = get_similar_account_pairs(posts, accounts, hashtag, min_similarity)\n",
        "\n",
        "    # Return the result as JSON\n",
        "    return jsonify(similar_account_pairs)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YUm9sbxG-SD"
      },
      "source": [
        "# Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704LJCDEHFQG"
      },
      "source": [
        "Given an API endpoint for our model, we want to be able to serve it in an arbitrary computing environment. Write a `requirements.txt` file and Dockerfile that could be used to containerize the model and access it via its API. You do _not_ have to build and run the container in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCf-9hR5ZLZT"
      },
      "outputs": [],
      "source": [
        "requirements.txt :\n",
        "\n",
        "flask\n",
        "rapidfuzz\n",
        "polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzZIxtEil1jH"
      },
      "outputs": [],
      "source": [
        "Dockerfile:\n",
        "\n",
        "\n",
        "FROM python:latest\n",
        "WORKDIR /app\n",
        "COPY . /app\n",
        "RUN pip install -r requirements.txt\n",
        "EXPOSE 5000\n",
        "ENV FLASK_APP=app.py\n",
        "CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnj4g_KRH5zV"
      },
      "source": [
        "# Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrYFpz7RH9D4"
      },
      "source": [
        "Suppose that we have found our model performs well in practice, and we now want to deploy it to Artemis, our SaaS offering for detecting, assessing, and mitigating disinformation and other online harms. In production, there is a significantly higher volume of data, both in terms of the number of posts and accounts, and the number of users who are making concurrent requests to the model.\n",
        "\n",
        "How would you ensure and evaluate that the model is performant in these circumstances? What monitoring may you want to put in place?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatyGA-AN5gH"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As soon as I deploy the model in QA/Staging environment. I would first consider creating some monitoring and have them in place.\n",
        "\n",
        "1. Response Time Monitoring: Firstly, Measure the response time of the API endpoint with different loads. Monitor the average response time, as well as the distribution of response times. identify any cases where there is a sudden increase in response time, which can indicate performance issues.\n",
        "\n",
        "2. Throughput Monitoring: I will identify the number of requests served per unit of time. This helps me in assessing the capacity of API endpoint and whether it can handle the expected workload.\n",
        "\n",
        "3. Error Rate Monitoring: Monitor the rate of errors returned by API endpoint. High error rates may indicate issues with the model inference, data processing, or infrastructure.\n",
        "\n",
        "4. Concurrency Monitoring: Identify the number of concurrent requests being processed by the endpoint. If the system is not able to process then we may consider horizontal scaling. We can consider deploying the endpoint in Kuberentes cluster that scales automatically\n",
        "\n",
        "Finally, I wll consider having tools like Grafana/Kibana in place to collect and visualize perfomance metrics. And have a alerting mechanism if there are any issues like 1. 500 Errors 2. Out of SLA etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "pXx3xtS5ssuz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}